### 1. 决策树/随机森林原理  
#### 1.1. 决策树基本流程  
决策树是数据挖掘与机器学习领域中一种非常重要的分类器，算法通过训练数据来构建一棵用于分类的树，从而对未知数据进行高效分类。
举例，某相亲网站通过调查相亲历史数据发现，女孩在实际相亲时有如下表现：  

序号 | 城市拥有房产 |婚姻历史（离过婚、单身）|年收入（单位：万元）|见面（是、否）
---|--- | --- | --- | ---
1 | 是 | 单身 | 12 | 是
2 | 否 | 单身 | 15| 是
3 | 是 | 离过婚 | 10 | 是
4|否|单身|18|是|
5|是|离过婚|25|是
6|是|单身|50|是
7|否|离过婚|35|是
8|是|离过婚|40|是
9|否|单身|60|是
10|否|离过婚|17|否

可以构建如下决策树：  
![image](http://note.youdao.com/yws/public/resource/1b9ef2c5d414f98f0c8bb01532772683/xmlnote/6EB08E76977F416E928B28B8B6B2FD53/1430)  

如果网站新注册了一个用户，他在城市无房产、年收入小于 35w 且离过婚，则可以预测女孩不会跟他见面。
决策树生成的基本过程：
1. 将所有记录看作是一个节点
2. 遍历每个变量的每种分割方式，找到最好的分割点
3. 利用分割点将记录分割成两个子结点 C1 和 C2
4. 对子结点 C1 和 C2 重复执行步骤 2）、3），直到满足特定条件为止。
可以看出，决策树的生成是一个递归过程，其中关键是第2步，找到最好的分割点，即划分选择。  
#### 1.2. 划分选择  
如果一个分割点能够将整个记录准确地分为两类，那该分割点就可以认为是最好的，此时被分成的两类是相对来说是最“纯”的。我们希望决策树的分支结点所包含的样本越纯越好，即结点的“纯度”越高越好。  
目前有三种纯度计算方式，分别是Gini不纯度、熵（Entropy）及错误率，它们的公式定义如下：  
![image](http://note.youdao.com/yws/public/resource/1b9ef2c5d414f98f0c8bb01532772683/xmlnote/F360929A211A439F9E0E5D555A05474F/1444)  
![image](http://note.youdao.com/yws/public/resource/1b9ef2c5d414f98f0c8bb01532772683/xmlnote/E527CD130D384030952D530514BF975A/1446)  
![image](http://note.youdao.com/yws/public/resource/1b9ef2c5d414f98f0c8bb01532772683/xmlnote/477807CF128043B9AB7B87E5425C54B4/1445)  
==variance==  
公式中的 P(i) 表示记录中第 i 类记录数占总记录数的比例，例如前面的女孩相亲例子可以根据见面或不见面分为两类，见面的记录占比数为 P(1)=9/10，不见面的记录占比为 P(2)=1/10。上面的三个公式均是值越大表示越“不纯”，值越小表示越“纯”。实际中最常用的是 Gini 不纯度公式。  
决策树的构建是一个递归的过程，理想情况下所有的记录都能被精确分类，即生成决策树叶节点都有确定的类型，但现实这种条件往往很难满足，这使得决策树在构建时可能很难停止。即使构建完成，也常常会使得最终的节点数过多，从而导致过度拟合（overfitting），因此在实际应用中需要设定停止条件，当达到停止条件时，直接停止决策树的构建。但这仍然不能完全解决过度拟合问题，过度拟合的典型表现是决策树对训练数据错误率很低，而对测试数据其错误率却非常高。    
过度拟合常见原因有：（1）训练数据中存在噪声；（2）数据不具有代表性。过度拟合的典型表现是决策树的节点过多，因此实际中常常需要对构建好的决策树进行枝叶裁剪（Prune Tree），但它不能解决根本问题，随机森林算法的出现能够较好地解决过度拟合问题。

#### 1.3. 集成学习方法
简单来说，集成学习方法就是基于其他的机器学习算法，并把它们有效的组合起来的一种机器学习算法。组合产生的算法相比其中任何一种算法模型更强大、准确。  
在MLlib中使用决策树作为基础模型。提供两种集成算法：随机森林和梯度提升树(GBTs)。两者之间主要差别在于每棵树训练的顺序。  
随机森林通过对数据随机采样来单独训练每一棵树。这种随机性也使得模型相对于单决策树更健壮，且不易在训练集上产生过拟合。    
GBTs则一次只训练一棵树，后面每一棵新的决策树逐步矫正前面决策树产生的误差。随着树的添加，模型的表达力也愈强。  
最后，两种方法都生成了一个决策树的权重集合。该集成模型通过组合每棵独立树的结果来进行预测。下图显示一个由3棵决策树集成的简单实例。      
![image](http://note.youdao.com/yws/public/resource/1b9ef2c5d414f98f0c8bb01532772683/xmlnote/100DE92A3A824A18882CEF6E9B178447/1463)    
在上述例子的回归集合中，每棵树都预测出一个实值。这些预测值被组合起来产生最终集成的预测结果。这里通过取均值的方法来取得最终的预测结果（当然不同的预测任务需要用到不同的组合算法）。

#### 1.4. 集成学习的分布式学习算法
在MLlib中，随机森林和GBTs的数据都是按实例（行）存储的。算法的实现以原始的决策树代码为基础，每棵决策树采用分布式学习。许多算法优化都是参考Google’s PLANET project。  
随机森林：随机森林中的每棵树都是单独训练，多棵树可以并行训练（除此之外，单独的每棵树的训练也可以并行化）。MLlib也确实是这么做的：根据当前迭代内存的限制条件，动态调整可并行训练的子树的数量。  
GBTs：因为GBTs只能一次训练一棵树，因此并行训练的粒度也只能到单棵树。  
在这里强调一下MLlib中用到的两项重要的优化技术：  
1.内存：随机森林使用一个不同的样本数据训练每一棵树。我们利用TreePoint这种数据结构来存储每个子采样的数据，替代直接复制每份子采样数据的方法，进而节省了内存。  
2.通信：尽管决策树经常通过选择树中每个决策点的所有功能进行训练，但随机森林则往往在每一个节点限制选择一个随机子集。MLlib的实现中就充分利用了这个子采样特点来减少通信：例如，若每个节点值用到1/3的特征，那么我们就会减少1/3的通信。

#### 1.5. 随机森林
由多个决策树构成的森林，算法分类结果由这些决策树投票得到，决策树在生成的过程当中分别在行方向和列方向上添加随机过程，行方向上构建决策树时采用放回抽样（bootstraping）得到训练数据，列方向上采用无放回随机抽样得到特征子集，并据此得到其最优切分点，这便是随机森林算法的基本原理。  
随机森林是一个组合模型，内部仍然是基于决策树，同单一的决策树分类不同的是，随机森林通过多个决策树投票结果进行分类，算法不容易出现过度拟合问题。   
### 2. 随机森林在分布式上的实现
随机森林算法在单机环境下很容易实现，但在分布式环境下特别是在Spark平台上，传统单机形式的迭代方式必须要进行相应改进才能适用于分布式环境，这是因为在分布式环境下，数据也是分布式的，算法设计不得当会生成大量的IO操作，例如频繁的网络数据传输（传输内容如连续变量的切分点选择需要排序、每个节点的特征选择需要广播），从而影响算法效率。 
MLlib中，split为划分点，对应二叉的决策树，bin为桶或者箱子数，一个split把数据集划分成2个桶，所以bin是split的2倍。  
maxBins：每个特征分裂时，最大划分(桶)数量  
maxDepth：树的最大高度  
Spark中的随机森林算法主要实现了以下优化策略：  
- 切分点抽样统计，如下图所示。在单机环境下的决策树对连续变量进行切分点选择时，一般是通过对特征点进行排序，然后取相邻两个数之间的点作为切分点，这在单机环境下是可行的，但如果在分布式环境下如此操作的话，会带来大量的网络传输操作，特别是当数据量达到PB级时，算法效率将极为低下。为避免该问题，Spark中的随机森林在构建决策树时，会对各分区采用一定的子特征策略进行抽样，然后生成各个分区的统计数据，并最终得到切分点。    
![image](http://note.youdao.com/yws/public/resource/1b9ef2c5d414f98f0c8bb01532772683/xmlnote/AFD92CF7593D40FB8450280A24B9F029/1490)
-  特征装箱（Binning），如下图所示。决策树的构建过程就是对特征的取值不断进行划分的过程，对于离散的特征，如果有M个值，最多有2^(M-1) - 1个划分。如果值是有序的，那么就最多M-1个划分。 比如年龄特征，有老，中，少3个值，如果无序有2^2-1=3个划分，即老|中，少；老，中|少；老，少|中。；如果是有序的，即按老，中，少的序，那么只有m-1个，即2种划分，老|中，少；老，中|少。 对于连续的特征，其实就是进行范围划分，而划分的点就是split（切分点），划分出的区间就是bin。对于连续特征，理论上split是无数的，在分布环境下不可能取出所有的值，因此它采用的是切点抽样统计方法。(从源代码里面看，是先对样本进行抽样，根据抽样样本值出现的次数进行排序，然后再采用分位数作为分割点候选。)  
![image](http://note.youdao.com/yws/public/resource/1b9ef2c5d414f98f0c8bb01532772683/xmlnote/831E670354A144A495F2ABB3F12D77B9/1492)
-  逐层训练（level-wise training），如下图所示。单机版本的决策树生成过程是通过递归调用（本质上是深度优先）的方式构造树，在构造树的同时，需要移动数据，将同一个子节点的数据移动到一起。 此方法在分布式数据结构上无法有效的执行，而且也无法执行，因为数据太大，无法放在一起，所以在分布式环境下采用的策略是逐层构建树节点（本质上是广度优先），这样遍历所有数据的次数等于所有树中的最大层数。 每次遍历时，只需要计算每个节点所有切分点统计参数，遍历完后，根据节点的特征划分，决定是否切分，以及如何切分。 
对于查找分割点split的并行操作，不是在一个node节点级别的并行，而是在树的level级别的并行，对于处于同一层次的节点，分割点的查找是同时并行操作的。也就是查找次数的复杂度依赖于层级L，而不是节点数2的L次方-1，这样可以减少IO、计算、以及相关的通信开销。  
![-](http://note.youdao.com/yws/public/resource/1b9ef2c5d414f98f0c8bb01532772683/xmlnote/F570636742324A05877A04790527A63B/1494)
- group-wise
分组计算bin，减少数据传输。每次迭代需要更多的计算和存储，但是减少迭代次数。
-  bin-wise
对于一些统计信息的计算都是在桶bins中进行的，由于预先对这些样本中bin的信息进行计算，勿需再每次迭代中计算，节省了计算开销。
- Aggregation over partitions
因为提前知道分片数，所以不用flatMap/reduceByKey操作，而是将所有分箱的聚合信息存到一个队列上，并利用RDD的aggregate方法大大减少通信开销。

基本的样本训练决策树的构建流程为：
寻找所有特征的可能的划分split以及桶信息bin，针对每次划分split，在spark executors上计算每个样本应该属于哪一个bin，后聚合每一个bin的统计信息，在Drivers上通过这些统计信息计算每次split的信息增益，并选择一个信息增益最大的分割split，按照该split对当前节点进行分割，直到满足终止条件。  
选择哪个特征进行划分，需要针对当前节点数据集合的每一个特征，求得信息增益，需要寻找切分点split来计算分区；对当前特征的每一个划分进行对应的信息增益计算，然后取当前特征的最大的信息增益值作为该特征的信息增益；依次求候选的特征的信息增益，再选取具有最大的信息增益的那个候选特征作为当前节点的属性选择度量即可。  
为了防止过拟合，需要考虑剪枝，这里采用的是前向剪枝，当任一以下情况发生，MLlib的决策树节点就终止划分，形成叶子节点：  
1.树高度达到maxDepth；    
2.minInfoGain 当前节点的所有属性分割带来的信息增益都比这个值要小；  
3.minInstancesPerNod 需要保证节点分割出的左右子节点的最少的样本数量达到这个值。	  
 
### 3. 数据流图
![image](http://note.youdao.com/yws/public/resource/39e26de526925ae3ab8772cf38d8521f/xmlnote/9ED7D97C155C4121BB4BAD2EEF18785A/1557)


---
**参考文献**  
[1] 机器学习.周志华  
[2] Spark随机森林算法原理、源码分析及案例实战  
[3] Scalable Distributed Decision Trees in Spark MLlib  
[4] https://databricks.com/blog/2014/09/29/scalable-decision-trees-in-mllib.html  
[5] https://databricks.com/blog/2015/01/21/random-forests-and-boosting-in-mllib.html  
[6] PLANET: Massively Parallel Learning of Tree Ensembles with MapReduce  
[7]  http://blog.csdn.net/yangbutao/article/details/45097025  
